## machine learning

model based on;
* task --> how the algorithm should process the data (classification, regression...)
* performance --> how good is the algorithm at the task
* experience
    * dataset
    * type of data --> real, discrete, mixed
    * feeding mode --> offline (batch), online (real time)

## terminology

supervised learning
* the given data is pre-classified, the algorithm will use you this to check how good it is doing, the training stops after reaching a certain performance
* used for predictions

unsupervised learning
* the given data is unclassified, the algorithm is expected to find regularities, patterns and extract information from it
* user for analysis, rule discovery

type of errors
* empirical error --> number of misclassifications
* TR - true error --> probability of misclassfication on real data classifications in any data
* overfitting if
    * ER(h) < ER(h') --> ER too high
    * TR(h) > TR(h') --> tr

spaces
* hypothesis space --> all the possible functions that the algorithm can generate
* instance space --> set of all possible instances or examples that can be encountered in a given problem
* vc-dimension --> the VC dimension of an hypothesis space H is the cardinality of the biggest instance space shattered by H
* shattering --> a set is shattered by a space if you can find an hyperplane to separate the set
* structural risk minimization --> higher VC dimension implies smaller ER and higher TR

glossary
* hyperparameter --> external parameter to the model (number of layers in NN)

model selection
* algorithm for choosing the best model at the end of the training --> hold out, k fold cross validation

## hold out

algorithm:
* remove a validation set (hold out) from the training set
* run the models on the remaining training set
* check the resulting model on the validation set

cons:
* the training set is no complete used for training

## k fold cross validation

algorithm:
* remove the validation set from the training set
* split the remaining training set in K parts
* train the model
* do it again k times with a different validation set
* calculate the man error

special case:
* leave one out --> k = training set size

## reinforcement learning

an agent is placed in a partial environment where it interacts to reach a goal in the best way possible

## markov decision process

defines a formal stochastic environment to place the agent, it's made of
* set of states S
* set of actions A
* a(s) --> which returns the possible actions in state s
* g(s, a) --> which returns the state from a state with action a
* r(s, a) --> which return the utility from a state with action a

the agent is expected to learn the policy that maximizes the value function

difference with supervised learning
* we know the function to maximize
* the learned function depends on r(s, a)

## V - state-value function

it's the expected value starting from a state and following a policy
* V(s) = r0 + r1 + ... + rn

the V function using the best policy is
* V*(s) = max_pi V(s)

only works if r(s, a) and g(s, a) are known

## Q - action-value function

it's the V* function, but with a given initial action
* Q(s, a) = r(s, a) + V*(g(s, a))

if the initial action of Q is optimal:
* Q(s, a*) = V*(s)

then Q can be written recursively
* Q(s, a) = r(s, a) + Q(s, a*)

## Q^ learning algorithm

```
while a certain error threshold is met
    a        = policy(s)
    s'       = g(s, a)

    Q^(s, a) = r(s, a) + (max_a Q(s', a))
    s        = s'
```

policy:
* exploration --> pick a random action --> slower to converge
* exploitation --> pick the action with the biggest Q-value in state s --> might not be optimal

proof - Q' converges to Q in deterministic envs:
* assume each pair (s, a) is performed infinite times
* Qn' is the table after n episodes
* max_err_n = max (Qn'(s, a) - Q(s, a)) // max err after each learning episode
* max_err_n is decreasing after each episode
    * Q_n''(s, a) - Q(s, a)
    * = (r + k * max Qn'(s', a')) - (r + k * max Q(s', a'))
    * = k * (max Qn'(s', a') - max Q(s', a'))
    * <= k*max ( Qn'(s', a') - Q(s', a') )
    * <= max_err_n


## TDL - temporal difference learning

to make Q algorithm work in non deterministic envs
* define V and Q by their expected values
* Qn(s, a) = r + k * (n * Q'(s', a') + (1-n) * max Q^n(s', a) )

props:
* might be faster than Q learning
* faster than V* for 0 < n < 1
